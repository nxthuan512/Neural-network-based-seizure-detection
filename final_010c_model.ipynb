{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model\n",
    "1. Load the postprocessing dataset\n",
    "2. Modify each label is for each sample, not each timestep\n",
    "3. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import argparse\n",
    "import datetime as dt\n",
    "import random\n",
    "import scipy.io as spio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    " \n",
    "import keras\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import GRU, LSTM, Dense, TimeDistributed, Flatten, Input, Dropout, Reshape, Concatenate\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score, fbeta_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xt_dataset:\n",
    "    def __init__(self, n_timestep, n_feature, dataset_id):\n",
    "        self.n_timestep = n_timestep\n",
    "        self.n_feature = n_feature\n",
    "        self.id = dataset_id\n",
    "    \n",
    "    # ==========================================================\n",
    "    # Load postprocessing dataset from file\n",
    "    # and store them in arrays X_train, y_train, X_test, y_test\n",
    "    # ==========================================================\n",
    "    def load(self, dir_in, fileID, batch_size, stateful, dnn):\n",
    "        print('---- INFO: Load dataset:')        \n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.X_val = []\n",
    "        self.X_test = []\n",
    "        self.y_test = []\n",
    "        \n",
    "        file_train = os.path.join(dir_in, 'Xtrain_' + str(fileID) + '.csv')\n",
    "        self.X_train = pd.read_csv(file_train, delimiter=\",\", header=None).values\n",
    "        file_train = os.path.join(dir_in, 'ytrain_' + str(fileID) + '.csv')\n",
    "        self.y_train = pd.read_csv(file_train, delimiter=\",\", header=None).values\n",
    "        \n",
    "        file_test = os.path.join(dir_in, 'Xtest_' + str(fileID) + '.csv')    \n",
    "        self.X_test = pd.read_csv(file_test, delimiter=\",\", header=None).values\n",
    "        file_test = os.path.join(dir_in, 'ytest_' + str(fileID) + '.csv')\n",
    "        self.y_test = pd.read_csv(file_test, delimiter=\",\", header=None).values\n",
    "        \n",
    "        # ------------------ \n",
    "        n_sample = int(self.y_train.shape[0])\n",
    "        print('------ Debug: n_sample =', n_sample)\n",
    "        \n",
    "        # Dense\n",
    "        if dnn == 'dense':\n",
    "            self.X_train = self.X_train.reshape(n_sample, self.n_feature) \n",
    "        # RNN\n",
    "        elif dnn == 'rnn':            \n",
    "            self.X_train = self.X_train.reshape(n_sample, self.n_timestep, self.n_feature)\n",
    "            if stateful == True:\n",
    "                n_sample = int(n_sample/batch_size) * batch_size\n",
    "                self.X_train = self.X_train[0:n_sample]\n",
    "                self.y_train = self.y_train[0:n_sample]\n",
    "        # CNN\n",
    "        elif dnn == 'cnn2d':\n",
    "            self.X_train = self.X_train.reshape(n_sample, self.n_timestep, 16, int(self.n_feature/16))\n",
    "        \n",
    "        self.y_train = self.y_train.reshape(n_sample, 1)\n",
    "\n",
    "        # ------------------\n",
    "        n_sample = int(self.y_test.shape[0])\n",
    "        print('------ Debug: n_sample =', n_sample)\n",
    "        \n",
    "        # Dense\n",
    "        if dnn == 'dense':\n",
    "            self.X_test = self.X_test.reshape(n_sample, self.n_feature) \n",
    "        # RNN\n",
    "        elif dnn == 'rnn':            \n",
    "            self.X_test = self.X_test.reshape(n_sample, self.n_timestep, self.n_feature)   \n",
    "            \n",
    "            if stateful == True:\n",
    "                n_sample = int(n_sample/batch_size) * batch_size\n",
    "                self.X_test = self.X_test[0:n_sample]\n",
    "                self.y_test = self.y_test[0:n_sample]\n",
    "        # CNN\n",
    "        elif dnn == 'cnn2d':\n",
    "            self.X_test = self.X_test.reshape(n_sample, self.n_timestep, 16, int(self.n_feature/16))\n",
    "\n",
    "        self.y_test = self.y_test.reshape(n_sample, 1)  \n",
    "\n",
    "        # ------------------\n",
    "        print('------ Shapes of X_train, y_train, X_test, y_test =', \\\n",
    "              self.X_train.shape, self.y_train.shape, self.X_test.shape, self.y_test.shape)     \n",
    "        \n",
    "# ===================================================\n",
    "# Test function\n",
    "#subject = xt_dataset(256, 16, 'Patient_1')  # timesteps, features\n",
    "#subject.load('D:\\Romanlab\\XT_DataSet\\dataset3\\RAW_EU_TESTCASE_5_16T', 0, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# https://github.com/keunwoochoi/keras_callbacks_example\n",
    "class xt_best_model(keras.callbacks.Callback):\n",
    "    \n",
    "    def config(self, batch_size, best_model_name, X, y):\n",
    "        self.max_auc = -1\n",
    "        self.max_sens = -1\n",
    "        self.max_f1score = -1\n",
    "        self.max_f2score = -1\n",
    "        self.min_loss = 1\n",
    "        self.prev_loss = 1\n",
    "        self.max_acc = 1\n",
    "        \n",
    "        self.test_train_max_auc = -1 \n",
    "        self.test_max_auc = -1\n",
    "        self.test_max_sens = -1\n",
    "        self.test_max_f1score = -1\n",
    "        self.test_max_f2score = -1\n",
    "        self.test_min_loss = 1\n",
    "        self.test_max_acc = 1\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.best_model_name = best_model_name\n",
    "        self.bs = batch_size\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.aucs = []\n",
    "        self.losses = []\n",
    "\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #self.losses.append(logs.get('loss'))\n",
    "        \n",
    "        # ----- Train -----\n",
    "        y_pred = self.model.predict(self.X, batch_size=self.bs)\n",
    "        \n",
    "        dim = self.y.shape \n",
    "        y_pred_ = y_pred.reshape(dim[0] * dim[1])\n",
    "        y_pred_ = np.round(y_pred_)\n",
    "        y_ = self.y.reshape(dim[0] * dim[1])        \n",
    "\n",
    "        current_auc = roc_auc_score(y_, y_pred_)        \n",
    "        current_tn, current_fp, current_fn, current_tp = confusion_matrix(y_, y_pred_).ravel()\n",
    "        current_sens = current_tp/(current_tp + current_fn)\n",
    "        \n",
    "        current_f1score = f1_score(y_, y_pred_)  \n",
    "        current_f2score = fbeta_score(y_, y_pred_, beta=2)  \n",
    "        \n",
    "        current_loss = logs.get('loss')\n",
    "        \n",
    "        \n",
    "        # Check AUC\n",
    "        #elif self.max_f2score == current_f2score:  \n",
    "        flag = 0\n",
    "        #if (self.max_auc < current_auc) or (self.max_auc == current_auc and self.min_loss > current_loss):\n",
    "        #if (current_loss < self.prev_loss) or (current_loss >= self.prev_loss and self.max_auc < current_auc):\n",
    "        #if (current_loss < self.min_loss) or ((current_loss / self.min_loss < 2) and current_auc >= self.max_auc):\n",
    "        #if (current_loss < self.min_loss or (current_loss >= self.min_loss and current_loss/self.min_loss < 1.1)) \\\n",
    "        #        and current_auc >= self.max_auc:\n",
    "        if (current_loss < self.min_loss) or (current_loss >= self.min_loss and self.max_auc < current_auc):\n",
    "            print('Update new train model: AUC =', self.max_auc, '-->', current_auc)\n",
    "            self.max_f2score = current_f2score\n",
    "            self.max_f1score = current_f1score\n",
    "            self.max_auc = current_auc\n",
    "            self.max_sens = current_sens\n",
    "            self.min_loss = current_loss\n",
    "            #self.model.save(self.best_model_name)  \n",
    "            flag = 1\n",
    "            \n",
    "        self.prev_loss = current_loss\n",
    "        \n",
    "        print('TRAIN: Current SENS =', current_sens, ' Max SENS =', self.max_sens, \\\n",
    "               ' Current AUC =', current_auc, ' Max AUC =', self.max_auc, \\\n",
    "                ' Current F1 =', current_f1score, ' Max F1 =', self.max_f1score, \\\n",
    "                 ' Current F2 =', current_f2score, ' Max F2 =', self.max_f2score, \\\n",
    "                  ' Current Loss =', current_loss, ' Min Loss =', self.min_loss \\\n",
    "             )\n",
    "        \n",
    "        # ---- Test ----\n",
    "        X = self.validation_data[0]\n",
    "        y = self.validation_data[1]\n",
    "        \n",
    "\n",
    "        y_pred = self.model.predict(X, batch_size=self.bs)\n",
    "        dim = y.shape \n",
    "        y_pred_ = y_pred.reshape(dim[0] * dim[1])\n",
    "        y_pred_ = np.round(y_pred_)\n",
    "        y_ = y.reshape(dim[0] * dim[1])        \n",
    "\n",
    "        current_loss = logs.get('val_loss')\n",
    "        current_acc = logs.get('val_acc')\n",
    "        \n",
    "        current_auc = roc_auc_score(y_, y_pred_)        \n",
    "        current_tn, current_fp, current_fn, current_tp = confusion_matrix(y_, y_pred_).ravel()\n",
    "        current_sens = current_tp/(current_tp + current_fn)\n",
    "        \n",
    "        current_f1score = f1_score(y_, y_pred_)  \n",
    "        current_f2score = fbeta_score(y_, y_pred_, beta=2) \n",
    "        \n",
    "        #if self.test_max_auc < current_auc:\n",
    "        #    print('Update new test AUC: AUC =', self.test_max_auc, '-->', current_auc)\n",
    "        #    self.test_max_f2score = current_f2score\n",
    "        #    self.test_max_f1score = current_f1score\n",
    "        #    self.test_max_auc = current_auc\n",
    "        #    self.test_max_sens = current_sens\n",
    "        #if flag == 1:\n",
    "        #    self.test_train_max_auc = current_auc\n",
    "        #    flag = 0\n",
    "        \n",
    "        #if (current_loss < self.prev_loss) or (current_loss >= self.prev_loss and self.max_auc < current_auc):\n",
    "        #if (current_loss < self.min_loss) or ((current_loss / self.min_loss < 2) and current_auc >= self.max_auc):\n",
    "        #if (current_loss < self.min_loss or (current_loss >= self.min_loss and current_loss/self.min_loss < 1.1)) \\\n",
    "        #        and current_auc >= self.max_auc:\n",
    "        #if (current_loss < self.test_min_loss) or (current_loss >= self.test_min_loss and self.test_max_auc < current_auc):\n",
    "        if self.test_max_auc < current_auc:\n",
    "            print('Update new test model: AUC =', self.test_max_auc, '-->', current_auc)\n",
    "            self.test_max_f2score = current_f2score\n",
    "            self.test_max_f1score = current_f1score\n",
    "            self.test_max_auc = current_auc\n",
    "            self.test_max_sens = current_sens\n",
    "            self.test_max_acc = current_acc\n",
    "            self.model.save(self.best_model_name)\n",
    "            \n",
    "        print('TEST: Current SENS =', current_sens, ' Max SENS =', self.test_max_sens, \\\n",
    "               ' Current AUC =', current_auc, ' Max AUC: ', self.test_max_auc, \\\n",
    "                ' Current F1 =', current_f1score, ' Max F1: ', self.test_max_f1score, \\\n",
    "                 ' Current F2 =', current_f2score, ' Max F2: ', self.test_max_f2score, \\\n",
    "                  ' Current Acc =', current_acc, ' Max Acc =', self.test_max_acc )\n",
    "                  #' AUC at train max =', self.test_train_max_auc)     \n",
    "            \n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 10\n",
    "- Two/Three hidden layers\n",
    "- Hyperparameters: batch_size, number of hidden units/layer 1, 2, 3\n",
    "- Class weight enable: 0/1\n",
    "- CNN algorithm as front-end\n",
    "- RNN algorithms: LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xt_model:\n",
    "    # ==========================================\n",
    "    def config(self, kfold, batch_size, rnn_algo, class_weight, n_hlayer, n_hunit, rnn_dropout):\n",
    "        print('---- INFO: Set model:')       \n",
    "        self.kfold = kfold\n",
    "        self.bs = batch_size\n",
    "        self.rnn_algo = rnn_algo   # LSTM, GRU\n",
    "        self.cw = class_weight   # 1: turn on, 0: turn off\n",
    "        self.n_hlayer = n_hlayer\n",
    "        \n",
    "        self.n_hunit = []\n",
    "        self.rnn_dropout = []\n",
    "        for i in range (0, self.n_hlayer):\n",
    "            self.n_hunit.append(n_hunit[i])\n",
    "            self.rnn_dropout.append(rnn_dropout[i])\n",
    "            \n",
    "        print('------ ' \\\n",
    "              'K-fold =', self.kfold, ', Batch size =', self.bs, \\\n",
    "              ', RNN algorithm =', self.rnn_algo, ', Class weight enable =', \\\n",
    "              self.cw, ', # hidden layers =', self.n_hunit, ', RNN dropout =', self.rnn_dropout),\n",
    "               \n",
    "    # ==========================================\n",
    "    # Model v0_7a\n",
    "    def run(self, dataset, k, stateful_rnn, dnn, model_suffix, X_ftest, y_ftest):\n",
    "        print('---- INFO: Running: id.k.bs.cw.hu =', dataset.id, k, self.bs, self.cw, self.n_hunit),\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Create model\n",
    "        # ----------------------------------------------------\n",
    "        if stateful_rnn == True:\n",
    "            input_layer = Input(batch_shape=(self.bs, dataset.n_timestep, dataset.n_feature))\n",
    "        else:\n",
    "            # Dense\n",
    "            if dnn == 'dense':\n",
    "                input_layer = Input(shape=(dataset.n_feature,))\n",
    "            # RNN\n",
    "            elif dnn == 'rnn':            \n",
    "                input_layer = Input(shape=(dataset.n_timestep, dataset.n_feature))\n",
    "            # CNN 2D\n",
    "            elif dnn == 'cnn2d':\n",
    "                input_layer = Input(shape=(dataset.n_timestep, 16, int(dataset.n_feature/16)))\n",
    "                                                                       \n",
    "        if dnn == 'dense':        \n",
    "            dense1 = Dense(dataset.n_feature, activation='relu')(input_layer) \n",
    "            dropout1 = Dropout(0.3)(dense1)\n",
    "            #dense3 = Dense(dataset.n_feature, activation='relu')(dropout1)\n",
    "            #dropout3 = Dropout(0.3)(dense3)\n",
    "            #dense4 = Dense(dataset.n_feature, activation='relu')(dropout3)\n",
    "            #dropout4 = Dropout(0.3)(dense4)\n",
    "            dense2 = Dense(1, activation='sigmoid')(dropout1)\n",
    "            model = Model(inputs=input_layer, outputs=dense2)\n",
    "            model.summary()\n",
    "            \n",
    "        elif dnn == 'rnn':\n",
    "            if stateful_rnn == True:\n",
    "                rnn2 = LSTM(32, stateful=True)(input_layer)\n",
    "                dropout2 = Dropout(0.4)(rnn2)\n",
    "                \n",
    "            elif self.rnn_algo == 'LSTM':\n",
    "                #rnn1 = LSTM(32, return_sequences=True)(input_layer)\n",
    "                #dropout1 = Dropout(0.4)(rnn1)\n",
    "                rnn2 = LSTM(80)(input_layer) # batche size 64, # timestep (1), # features (80)\n",
    "                dropout2 = Dropout(0.4)(rnn2)\n",
    "            #elif self.rnn_algo == 'GRU':\n",
    "            #    rnn1 = GRU(32)(maxpool11)\n",
    "            #else:\n",
    "            #    rnn1 = SimpleRNN(32)(maxpool11)        \n",
    "            dense1 = Dense(1, activation='sigmoid')(dropout2)\n",
    "            model = Model(inputs=input_layer, outputs=dense1)\n",
    "            model.summary()\n",
    "            \n",
    "        \n",
    "        elif dnn == 'cnn2d':\n",
    "            #conv1 = Conv2D(16, kernel_size=(1, 1), activation='relu')(input_layer) # (16 x 1) x 6 -> 16 x 16\n",
    "            #conv2 = Conv2D(32, kernel_size=(1, 2), strides=(1, 2), activation='relu')(conv1) # 16 x 16 -> 8 x 32\n",
    "            #conv3 = Conv2D(64, kernel_size=(1, 2), strides=(1, 2), activation='relu')(conv2) # 8 x 32 -> 4 x 64\n",
    "            #conv4 = Conv2D(128, kernel_size=(1, 2), strides=(1, 2), activation='relu')(conv3) # 4 x 64 -> 2 x 128\n",
    "            \n",
    "            conv1 = Conv2D(32, kernel_size=(1, 1), activation='relu')(input_layer) # 32 x 16            \n",
    "            conv2 = Conv2D(32, kernel_size=(1, 2), activation='relu')(conv1) # 32 x 15            \n",
    "            conv3 = Conv2D(32, kernel_size=(1, 2), activation='relu')(conv2) # 32 x 14\n",
    "            maxpool3 = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(conv3) # 32 x 7\n",
    "            \n",
    "            conv4 = Conv2D(64, kernel_size=(1, 2), activation='relu')(maxpool3) # 64 x 6\n",
    "            conv5 = Conv2D(64, kernel_size=(1, 2), activation='relu')(conv4) # 64 x 5\n",
    "            conv6 = Conv2D(64, kernel_size=(1, 2), activation='relu')(conv5) # 64 x 4\n",
    "            maxpool6 = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(conv6) # 64 x 2\n",
    "            \n",
    "            flatten1 = Flatten()(maxpool6) # 128\n",
    "            dropout1 = Dropout(0.4)(flatten1)\n",
    "            dense1 = Dense(128, activation='relu')(dropout1)\n",
    "            dropout2 = Dropout(0.4)(dense1)\n",
    "            dense2 = Dense(1, activation='sigmoid')(dropout2)\n",
    "            model = Model(inputs=input_layer, outputs=dense2)\n",
    "            model.summary()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ==========================================\n",
    "        # define roc_callback, inspired by https://github.com/keras-team/keras/issues/6050#issuecomment-329996505\n",
    "        # Ref: https://stackoverflow.com/questions/41032551/\n",
    "        # how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "        #def recall(y_true, y_pred):\n",
    "        #    # reset the local variables\n",
    "        #    #tf.local_variables_initializer()\n",
    "        #    # any tensorflow metric\n",
    "        #    dim = tf.shape(y_true)\n",
    "        #    y_true = tf.reshape(y_true, [(dim[0] * dim[1]), dim[2]])\n",
    "        #    y_pred = tf.round(tf.reshape(y_pred, [(dim[0] * dim[1]), dim[2]]))\n",
    "        #    #value, update_op = tf.metrics.auc(predictions=y_pred, labels=y_true)\n",
    "        #    value, update_op = tf.metrics.recall(predictions=y_pred, labels=y_true)\n",
    "            \n",
    "        #    # find all variables created for this metric\n",
    "        #    metric_vars = [i for i in tf.local_variables() if 'recall' in i.name.split('/')[1]]\n",
    "\n",
    "        #    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "        #    # They will be initialized for new session.\n",
    "        #    for v in metric_vars:\n",
    "        #        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "        #    # force to update metric values\n",
    "        #    with tf.control_dependencies([update_op]):\n",
    "        #        value = tf.identity(value)\n",
    "        #        return value\n",
    "    \n",
    "        # ==========================================\n",
    "        #adam = optimizers.Adam(lr=0.0001)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[recall])\n",
    "        \n",
    "        # Calculate class weight\n",
    "        # Ref: https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-\n",
    "        # for-imbalanced-classes-in-keras\n",
    "        if self.cw == 1:\n",
    "            dim = dataset.y_train.shape \n",
    "            y_train_ = dataset.y_train.reshape(dim[0] * dim[1])\n",
    "        else:\n",
    "            y_train_ = [0, 1]\n",
    "        \n",
    "        class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train_), y_train_)\n",
    "        #class_weights = [1, 10]\n",
    "        print('------ Class_weights =', class_weights)\n",
    "        \n",
    "        # ----------------------------------------------------\n",
    "        # Create call_back\n",
    "        # ----------------------------------------------------\n",
    "        now = datetime.datetime.now()\n",
    "        sub_name = 'date--id.k.bs.cw.hu1.hu2.hu3--' \\\n",
    "            + (now.strftime(\"%Y.%m.%d.%H.%M--\")) + dataset.id + '.' \\\n",
    "            + str(k) + '.' + str(self.bs) + '.' + str(self.cw) + '.' \\\n",
    "            + str(self.n_hunit[0]) + '.' + str(self.n_hunit[1]) + '.' + str(self.n_hunit[2])\n",
    "        \n",
    "        best_model_name = './Model_10a/best-model--' + model_suffix\n",
    "        graph_name = './Graph_10a/' + sub_name\n",
    "\n",
    "        # train LSTM\n",
    "        # Ref: https://keras.io/callbacks/\n",
    "        my_callback = xt_best_model()\n",
    "        my_callback.config(self.bs, best_model_name, dataset.X_train, dataset.y_train)\n",
    "         \n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(monitor='loss', #'val_loss', \n",
    "                                          patience=5000, \n",
    "                                          min_delta=0.001,\n",
    "                                          verbose=1,\n",
    "                                          mode='min'),\n",
    "            #keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "            #                                monitor='val_recall', #'val_loss', \n",
    "            #                                save_best_only=True, \n",
    "            #                                verbose=1,\n",
    "            #                                mode='max'),\n",
    "            keras.callbacks.TensorBoard(log_dir=graph_name, \n",
    "                                        histogram_freq=0, \n",
    "                                        write_graph=True, \n",
    "                                        write_images=True,\n",
    "                                        write_grads=False),\n",
    "            my_callback\n",
    "        ]\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Train model\n",
    "        # ----------------------------------------------------\n",
    "        # Ref: https://keras.io/models/model/\n",
    "        history = model.fit(dataset.X_train, \n",
    "                              dataset.y_train, \n",
    "                              epochs=3000, \n",
    "                              batch_size=self.bs, \n",
    "                              verbose=1, \n",
    "                              shuffle=False,\n",
    "                              #shuffle='batch',\n",
    "                              callbacks=callbacks,\n",
    "                              class_weight=class_weights,\n",
    "                              validation_data=(dataset.X_test, dataset.y_test))\n",
    "                              #validation_split=0.2)\n",
    "#         for i in range (0, 5):\n",
    "#             history = model.fit(dataset.X_train, \n",
    "#                                   dataset.y_train, \n",
    "#                                   epochs=5, \n",
    "#                                   batch_size=self.bs, \n",
    "#                                   verbose=1, \n",
    "#                                   #shuffle=False,\n",
    "#                                   shuffle='batch',\n",
    "#                                   callbacks=callbacks,\n",
    "#                                   class_weight=class_weights,\n",
    "#                                   validation_data=(dataset.X_test, dataset.y_test))\n",
    "#                                   #validation_split=0.2)\n",
    "\n",
    "#             history = model.fit(dataset.X_train, \n",
    "#                                   dataset.y_train, \n",
    "#                                   epochs=195, \n",
    "#                                   batch_size=self.bs, \n",
    "#                                   verbose=1, \n",
    "#                                   shuffle=False,\n",
    "#                                   #shuffle='batch',\n",
    "#                                   callbacks=callbacks,\n",
    "#                                   class_weight=class_weights,\n",
    "#                                   validation_data=(dataset.X_test, dataset.y_test))\n",
    "#                                   #validation_split=0.2)\n",
    "                \n",
    "        # plot history\n",
    "        #pyplot.plot(history.history['loss'], label='train')\n",
    "        #pyplot.plot(history.history['val_loss'], label='test')\n",
    "        #pyplot.legend()\n",
    "        #pyplot.show()\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Save results\n",
    "        # ----------------------------------------------------\n",
    "        #print('my_callback.losses=', my_callback.losses)\n",
    "        #print('my_callback.aucs=', my_callback.aucs)\n",
    "\n",
    "        self.train_loss, self.train_acc, self.test_loss, self.test_acc = [0, 0, 0, 0]\n",
    "        self.tp, self.fn, self.tn, self.fp = [0, 0, 0, 0]\n",
    "        self.sens, self.spec, self.auc, self.f1score, self.f2score = [0, 0, 0, 0, 0]\n",
    "        \n",
    "        # Load best model\n",
    "        #best_model = load_model(best_model_name, custom_objects={'recall': recall})\n",
    "        best_model = load_model(best_model_name)\n",
    "        \n",
    "        # Loss, Accuracy\n",
    "        # Ref: https://keras.io/models/model/\n",
    "        self.train_loss, self.train_acc = best_model.evaluate(dataset.X_train,\n",
    "                                                              dataset.y_train,\n",
    "                                                              batch_size=self.bs)\n",
    "        self.test_loss, self.test_acc = best_model.evaluate(dataset.X_test, \n",
    "                                                            dataset.y_test,\n",
    "                                                            batch_size=self.bs)\n",
    "        \n",
    "        # Ref: https://keras.io/models/model/\n",
    "        y_pred = best_model.predict(dataset.X_test, \n",
    "                                    batch_size=self.bs, \n",
    "                                    verbose=0, \n",
    "                                    steps=None)\n",
    "\n",
    "        # Flatten y_test and y_pred\n",
    "        #print('------ Debug: y_test.shape =', y_test.shape)\n",
    "        dim = dataset.y_test.shape \n",
    "        y_pred_ = y_pred.reshape(dim[0] * dim[1])\n",
    "        y_pred_ = np.round(y_pred_)\n",
    "        y_test_ = dataset.y_test.reshape(dim[0] * dim[1])\n",
    "\n",
    "        # Save pred, test to file\n",
    "        result_name = './Results_10a/test-pred--' + sub_name\n",
    "        np.savetxt(result_name, [y_test_, y_pred_], delimiter=' ', fmt = '%.1f')    \n",
    "\n",
    "        # TN, FP, FN, TP, AUC, sensitivity, specificity, AUC\n",
    "        # Ref: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "        self.tn, self.fp, self.fn, self.tp = confusion_matrix(y_test_, y_pred_).ravel()\n",
    "        self.sens = self.tp/(self.tp + self.fn)\n",
    "        self.spec = self.tn/(self.fp + self.tn)\n",
    "        self.auc = roc_auc_score(y_test_, y_pred_)\n",
    "        self.f1score = f1_score(y_test_, y_pred_)\n",
    "        self.f2score = fbeta_score(y_test_, y_pred_, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------------------------------------------\n",
    "# User setting\n",
    "filter_en = 0\n",
    "stateful_rnn = False # decide LSTM stateful or stateless\n",
    "test_subject_id = 'Patient_1' # Dog_1, ..., Patient_1, ... , Dog_ (full), Patient_ (full)\n",
    "model_suffix = '1E_T'\n",
    "dir_in = os.path.join('D:\\Romanlab\\XT_DataSet\\dataset3', 'RAW_EU_TESTCASE_' + model_suffix)  \n",
    "\n",
    "# ------------------------------------\n",
    "# Initialize\n",
    "if filter_en == 1:\n",
    "    n_band = 5\n",
    "else:\n",
    "    n_band = 1\n",
    "#\n",
    "test_subject = xt_dataset(1, 80, test_subject_id) # timesteps, features\n",
    "model = xt_model()\n",
    "\n",
    "# ------------------------------------ \n",
    "# Results are stored in result array that contains 13 dimensions:\n",
    "# train_loss, train_acc, test_loss, test_acc, tp, fn, tn, fp, sensitivity, specificity, auc, f1-score, f2-score\n",
    "results = []\n",
    "\n",
    "kfold = 5\n",
    "bs =  [ 64]\n",
    "hu1 = [ 8, 16, 32, 64, 32, 36]\n",
    "hu2 = [ 8, 16, 32, 64, 36, 8]\n",
    "hu3 = [ 8, 16, 32]\n",
    "dnn = 'dense'\n",
    "\n",
    "X_ftest = []\n",
    "y_ftest = []\n",
    "#X_ftest = np.loadtxt(os.path.join(dir_in, 'X_96_96'), delimiter=' ')\n",
    "#y_ftest = np.loadtxt(os.path.join(dir_in, 'y_96_96'))\n",
    "#dim = X_ftest.shape\n",
    "#X_ftest = X_ftest.reshape(int(dim[0]/256), 256, 16)\n",
    "#y_ftest = y_ftest.reshape(y_ftest.shape[0], 1)\n",
    "#print('X_ftest.shape, y_ftest.shape =', X_ftest.shape, y_ftest.shape)\n",
    "\n",
    "for i in range (2, 3):    \n",
    "    # kfold, batch_size, rnn_algo, class_weight, n_hlayer, n_hunit\n",
    "    model.config(kfold, bs[0], 'LSTM', 1, 3, [hu1[i], 0, 0], [0, 0, 0])\n",
    "    \n",
    "    for j in range (0, 1):  \n",
    "        for k in range (0, 3):  \n",
    "            #\n",
    "            test_subject.load(dir_in, j, bs[0], stateful_rnn, dnn)\n",
    "            model.run(test_subject, j, stateful_rnn, dnn, \\\n",
    "                      (model_suffix + '_1' + dnn + '_' + str(j) + str(k)), X_ftest, y_ftest)\n",
    "\n",
    "            #\n",
    "            result = [model.train_loss, model.train_acc, model.test_loss, model.test_acc, \\\n",
    "                      model.tp, model.fn, model.tn, model.fp, \\\n",
    "                      model.sens, model.spec, model.auc, model.f1score, model.f2score]\n",
    "            results.append(result)\n",
    "\n",
    "            #\n",
    "            print(result)\n",
    "            np.savetxt('./Results_10a/20180924_result09a.csv', results, delimiter=',')   # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate full model with each test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xt_k_of_n(y_pred, y_test, tsuf, k, n):\n",
    "    print('------ Debug: tsuf, k, n =', tsuf, k, n)\n",
    "    \n",
    "    rec_73 = [3105, 3226]\n",
    "    rec_75 = [3367, 3481]\n",
    "    rec_96 = [2718, 2836]\n",
    "    rec_101 = [623, 744]\n",
    "    rec_120 = [2041, 2198]\n",
    "    rec_122 = [324, 447]\n",
    "    rec_124 = [2772, 2893]\n",
    "    rec_126 = [1791, 1935]\n",
    "    \n",
    "    if tsuf == '73_73':\n",
    "        rec = rec_73\n",
    "    elif tsuf == '75_75':\n",
    "        rec = rec_75\n",
    "    elif tsuf == '96_96':\n",
    "        rec = rec_96\n",
    "    elif tsuf == '101_101':\n",
    "        rec = rec_101\n",
    "    elif tsuf == '120_120':\n",
    "        rec = rec_120\n",
    "    elif tsuf == '122_122':\n",
    "        rec = rec_122\n",
    "    elif tsuf == '124_124':\n",
    "        rec = rec_124\n",
    "    elif tsuf == '126_126':\n",
    "        rec = rec_126\n",
    "    else:\n",
    "        rec = [0, 0]\n",
    "    \n",
    "    M = y_pred.shape[0]\n",
    "    y_pred_tmp = np.zeros(M)\n",
    "    if k > 1 and n > 1:\n",
    "#         print('------ Debug: FILTER')\n",
    "        for i in range (0, M):\n",
    "            if (i >= n-1) and (np.sum(y_pred[i-n:i]) >= k):\n",
    "                y_pred_tmp[i] = 1\n",
    "    else:\n",
    "        y_pred_tmp = y_pred\n",
    "    \n",
    "#     print('------ Debug: LATENCY')\n",
    "    latency = 0\n",
    "    if rec != [0, 0]:\n",
    "#         print('------ Debug: ', rec)\n",
    "        for i in range(rec[0], rec[1]):\n",
    "#             print('------ Debug: ', y_test[i], y_pred_tmp[i])\n",
    "            if (y_test[i] == 1) and (y_pred_tmp[i] == 0):\n",
    "                latency = latency + 1\n",
    "            elif (y_test[i] == y_pred_tmp[i]):\n",
    "                break;\n",
    "    \n",
    "    print('------ Debug: FP')\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_tmp, labels=[0,1]).ravel()\n",
    "\n",
    "    return latency, fp, y_pred_tmp\n",
    "\n",
    "def xt_eval_subset(X_test, y_test, batch_size, tsuf, k, n, best_model_name):        \n",
    "    # Load best model\n",
    "    best_model = load_model(best_model_name)\n",
    "\n",
    "    # Loss, Accuracy\n",
    "    # Ref: https://keras.io/models/model/\n",
    "    test_loss, test_acc = best_model.evaluate(X_test, \n",
    "                                              y_test,\n",
    "                                              batch_size=batch_size)\n",
    "\n",
    "    # Ref: https://keras.io/models/model/\n",
    "    y_pred = best_model.predict(X_test, \n",
    "                                verbose=0,\n",
    "                                batch_size=batch_size,\n",
    "                                steps=None)\n",
    "\n",
    "    # Flatten y_test and y_pred\n",
    "    #print('------ Debug: y_test.shape =', y_test.shape)\n",
    "    dim = y_test.shape \n",
    "    y_pred_ = y_pred.reshape(dim[0] * dim[1])\n",
    "    \n",
    "    np.savetxt('./Results_10a/tmp.csv', y_pred_, delimiter=',', fmt='%.4f')        \n",
    "        \n",
    "    y_pred_ = np.round(y_pred_)\n",
    "    y_test_ = y_test.reshape(dim[0] * dim[1])\n",
    "    \n",
    "    # ==============================================\n",
    "    fig=plt.figure(figsize=(18, 6), dpi= 100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    start = 1\n",
    "    end = 3600 #y_test.shape[0]\n",
    "    t = range (start, end)\n",
    "\n",
    "    print('=== Raw results === ')\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    #plt.fill_between(t, y_test[start:end], facecolor='yellow')\n",
    "    plt.fill_between(t, y_pred_[start:end], facecolor='g', step=\"post\", label='y_pred')\n",
    "    plt.plot(t, y_test_[start:end], color='r', drawstyle='steps-post', label='y_true') \n",
    "    #plt.plot(t, y_pred[start:end], color='b', linestyle=':')\n",
    "    plt.title('y_test vs y_pred')\n",
    "    plt.ylabel('Class')\n",
    "    plt.legend()\n",
    "    #fig.savefig('D:\\\\Romanlab\\\\XT_Projects\\\\04_rnn\\\\report_20180819\\\\results\\\\REC75_LSTM_NoShuffle.pdf')\n",
    "    # ==============================================\n",
    "\n",
    "\n",
    "    # TN, FP, FN, TP, AUC, sensitivity, specificity, AUC\n",
    "    # Ref: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_, y_pred_).ravel()\n",
    "    sens = tp/(tp + fn)\n",
    "    spec = tn/(fp + tn)\n",
    "    auc = roc_auc_score(y_test_, y_pred_)\n",
    "    \n",
    "    print('test_loss, test_acc, tp, fn, tn, fp, sens, spec, auc =', test_loss, test_acc, tp, fn, tn, fp, sens, spec, auc)\n",
    "    \n",
    "    latency, fp, y_pred_tmp = xt_k_of_n(y_pred_, y_test_, tsuf, k, n)\n",
    "    print('latency, fp =', latency, fp)\n",
    "    \n",
    "#     plt.subplot(2, 1, 2)\n",
    "#     #plt.fill_between(t, y_test[start:end], facecolor='yellow')\n",
    "#     plt.fill_between(t, y_pred_tmp[start:end], facecolor='g', step=\"post\", label='y_pred')\n",
    "#     plt.plot(t, y_test_[start:end], color='r', drawstyle='steps-post', label='y_true') \n",
    "#     #plt.plot(t, y_pred[start:end], color='b', linestyle=':')\n",
    "#     plt.title('y_test vs y_pred_tmp')\n",
    "#     plt.ylabel('Class')\n",
    "#     plt.legend()\n",
    "    \n",
    "    return test_loss, test_acc, tp, fn, tn, fp, sens, spec, auc \n",
    "       \n",
    "# ==================================================\n",
    "# Test function\n",
    "# Test function\n",
    "model_suffix = '1E_T'\n",
    "\n",
    "if model_suffix == '1_T':\n",
    "    test_suffix = ['73_73']\n",
    "elif model_suffix == '2_T':\n",
    "    test_suffix = ['75_75']\n",
    "elif model_suffix == '3_T':\n",
    "    test_suffix = ['96_96']\n",
    "elif model_suffix == '4_T':\n",
    "    test_suffix = ['101_101']\n",
    "elif model_suffix == '5_T':\n",
    "    test_suffix = ['120_120']\n",
    "elif model_suffix == '6_T':\n",
    "    test_suffix = ['122_122']\n",
    "elif model_suffix == '7_T':\n",
    "    test_suffix = ['124_124']\n",
    "elif model_suffix == '8_T':\n",
    "    test_suffix = ['126_126']\n",
    "elif model_suffix == '1D_T' or model_suffix == '2D_T' or model_suffix == '3D_T':\n",
    "    test_suffix = ['122_122', '124_124', '126_126']\n",
    "elif model_suffix == '1E_T' or model_suffix == '2E_T' or model_suffix == '3E_T':\n",
    "    test_suffix = ['122_122', '124_124', '126_126']\n",
    "#test_suffix = ['148_148']\n",
    "\n",
    "print('INFO: Start')\n",
    "\n",
    "n_channel = 80\n",
    "n_timestep = 1\n",
    "batch_size = 64\n",
    "dnn = 'dense'\n",
    "k, n = 3, 5\n",
    "\n",
    "# ---------------------\n",
    "results = []\n",
    "for tsuf in test_suffix:\n",
    "    file_Xin = os.path.join('D:\\Romanlab\\XT_DataSet\\dataset3\\RAW_EU_TEST_A', 'X_' + tsuf + '.csv')\n",
    "    file_yin = os.path.join('D:\\Romanlab\\XT_DataSet\\dataset3\\RAW_EU_TEST_A', 'y_' + tsuf + '.csv')\n",
    "        \n",
    "    #\n",
    "    X_test = pd.read_csv(file_Xin, delimiter=\",\", header=None).values\n",
    "    y_test = pd.read_csv(file_yin, delimiter=\",\", header=None).values\n",
    "\n",
    "    #\n",
    "    dim = X_test.shape[0]\n",
    "    X_test = X_test.reshape((int(dim/n_timestep), n_timestep, n_channel))\n",
    "    y_test = y_test.reshape((int(dim/n_timestep), 1))\n",
    "\n",
    "    #\n",
    "    for msuf in range (0, 3):\n",
    "        file_Xmean = os.path.join('D:\\Romanlab\\XT_DataSet\\dataset3\\RAW_EU_TESTCASE_' + model_suffix, 'Xmean_0.csv')\n",
    "        file_Xstd = os.path.join('D:\\Romanlab\\XT_DataSet\\dataset3\\RAW_EU_TESTCASE_' + model_suffix, 'Xstd_0.csv')\n",
    "        \n",
    "        #\n",
    "        X_mean = pd.read_csv(file_Xmean, delimiter=\",\", header=None).values \n",
    "        X_mean = X_mean.T\n",
    "        X_std = pd.read_csv(file_Xstd, delimiter=\",\", header=None).values\n",
    "        X_std = X_std.T\n",
    "        \n",
    "        X_test_tmp = (X_test - X_mean) / X_std\n",
    "        \n",
    "        #np.savetxt('D:\\Romanlab\\XT_DataSet\\dataset3\\RAW_EU_TESTCASE_1_16T\\\\tmp', X_test_tmp, delimiter=' ', fmt='%.2f')\n",
    "        if dnn == 'rnn':\n",
    "            X_test_tmp = X_test_tmp.reshape(int(dim/n_timestep), n_timestep, n_channel)\n",
    "        elif dnn == 'dense':\n",
    "            X_test_tmp = X_test_tmp.reshape(dim, n_channel)\n",
    "        elif dnn == 'cnn2d':\n",
    "            X_test_tmp = X_test_tmp.reshape(int(dim/n_timestep), n_timestep, 16, int(n_channel/16))\n",
    "        \n",
    "        #print(X_mean, X_std, X_test)\n",
    "    \n",
    "        print('--- Debug: tsuf, msuf =', tsuf, msuf)\n",
    "        result = xt_eval_subset(X_test_tmp, y_test, batch_size, tsuf, k, n, \\\n",
    "                       './Model_10a/best-model--' + model_suffix + '_1' + dnn + '_0' + str(msuf)) \n",
    "                    #'./Model_10a/best-model--2_T_rnn_00_shuffle_False') \n",
    "        results.append(result)\n",
    "\n",
    "np.savetxt('./Results_10a/20180805_result08a.csv', results, delimiter=',')        \n",
    "print('INFO: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False)\n",
    "print(X)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, test_index in kf.split(X): \n",
    "    X_train.append(X[train_index])\n",
    "    y_train.append(y[train_index])   \n",
    "    X_test.append(X[test_index])\n",
    "    y_test.append(y[test_index])\n",
    "\n",
    "print('----------')\n",
    "print(X_train[4])\n",
    "print(X_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)                                      \n",
    "X_scaled = scaler.transform(X_train)      \n",
    "print(scaler)\n",
    "print(scaler.mean_)                                      \n",
    "print(scaler.scale_) \n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
